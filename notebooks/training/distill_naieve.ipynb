{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b1d74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from transformers import AutoTokenizer, BartForConditionalGeneration\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from torch.functional import F\n",
    "from tqdm import tqdm\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "\n",
    "from src.data_utils import get_sample_from_row_original, filter_irrelevant, prune_frequent_samples\n",
    "from src.inference_utils import predict\n",
    "from src.metrics import lemmatization_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf2a2e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "CURDIR = Path.cwd()\n",
    "\n",
    "DATADIR = CURDIR / \"data\" / \"original\"\n",
    "assert DATADIR.exists()\n",
    "\n",
    "MODELS_DIR = CURDIR / \"models\"\n",
    "assert MODELS_DIR.exists()\n",
    "\n",
    "TEACHER_ID = MODELS_DIR / 'baseline'\n",
    "assert TEACHER_ID.exists()\n",
    "\n",
    "STUDENT_ID = MODELS_DIR / \"checkpoint_120226\"\n",
    "assert STUDENT_ID.exists()\n",
    "\n",
    "RESULT_MODEL_DIR = MODELS_DIR / \"checkpoint_120226_2\"\n",
    "if not RESULT_MODEL_DIR.exists():\n",
    "    RESULT_MODEL_DIR.mkdir()\n",
    "RESULT_MODEL_PATH = RESULT_MODEL_DIR / \"model.pt\"\n",
    "\n",
    "MAX_LENGTH = 512\n",
    "DEVICE = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0e9c96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(DATADIR / \"train.csv\", index_col=0, sep=\"\\t\")\n",
    "df_dev = pd.read_csv(DATADIR / \"dev.csv\", index_col=0, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ec6bf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(TEACHER_ID)\n",
    "teacher = BartForConditionalGeneration.from_pretrained(TEACHER_ID).to(\"cuda\")\n",
    "student = BartForConditionalGeneration.from_pretrained(STUDENT_ID).to(\"cuda\")\n",
    "\n",
    "teacher.eval()\n",
    "for param in teacher.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b2b0088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2150060, 7), (255992, 7))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"sample\"] = df_train.apply(lambda row: get_sample_from_row_original(row)[0], axis=1)\n",
    "df_dev[\"sample\"] = df_dev.apply(lambda row: get_sample_from_row_original(row)[0], axis=1)\n",
    "\n",
    "df_train.shape, df_dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25f349c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = filter_irrelevant(df_train)\n",
    "df_dev = filter_irrelevant(df_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4618ba11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2135295, 7), (254996, 7))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape, df_dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f767283",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = prune_frequent_samples(df_train)  # здесь подрезаем несбалансированное начальное распределение\n",
    "df_dev = df_dev.drop_duplicates(subset=[\"sample\"])  # отсюда просто удаляем все дубли чтобы честно мериться"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b6ea04b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((958668, 8), (52827, 7))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape, df_dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3dc2c6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[[\"sample\", \"lemma\"]]\n",
    "df_dev = df_dev[[\"sample\", \"lemma\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5de799b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = Dataset.from_pandas(\n",
    "    df_train[[\"sample\", \"lemma\"]],\n",
    ").rename_columns({\n",
    "    \"sample\": \"source\",\n",
    "    \"lemma\": \"target\",\n",
    "}).shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c356642a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples,):\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"source\"],\n",
    "        max_length=70,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "    )\n",
    "\n",
    "    labels = tokenizer(\n",
    "        examples[\"target\"],\n",
    "        max_length=70,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "    )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68c97c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 958668/958668 [01:03<00:00, 15208.84 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_train = train.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    remove_columns=train.column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37749799",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    "    label_pad_token_id=-100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba1e837f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 56  # 56? 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24dbf623",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    tokenized_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac4ac38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KLDivAndCELoss(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.kldiv = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "        self.alpha = kwargs.get(\"alpha\", 0.5)\n",
    "        self.antialpha = 1 - self.alpha\n",
    "        self.temperature = kwargs.get(\"temperature\", 1.0)\n",
    "\n",
    "        self.ce = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "    def forward(self, student_logits, teacher_logits, targets=None):\n",
    "\n",
    "        _, _, vocab_size = student_logits.shape\n",
    "\n",
    "        student_log_probs = F.log_softmax(student_logits / self.temperature, dim=-1)\n",
    "        teacher_probs = F.softmax(teacher_logits / self.temperature, dim=-1) #* (self.temperature ** 2)\n",
    "\n",
    "        kd_loss = self.kldiv(student_log_probs, teacher_probs)  # маскировать PAD?\n",
    "\n",
    "        ## плохо работает с таргетами\n",
    "\n",
    "        # student_logits_flat = student_logits.contiguous().view(-1, vocab_size)\n",
    "        # targets_flat = targets.contiguous().view(-1)\n",
    "\n",
    "        # ce_loss = self.ce(student_logits_flat, targets_flat)\n",
    "\n",
    "        # result = self.alpha * kd_loss + (1 - self.alpha) * ce_loss\n",
    "\n",
    "        result = kd_loss\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b109bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distill_batch(\n",
    "    teacher: BartForConditionalGeneration,\n",
    "    student: BartForConditionalGeneration,\n",
    "    batch,\n",
    "    criterion:KLDivAndCELoss,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        teacher_generation = teacher.generate(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            max_length=32,\n",
    "            num_beams=1,\n",
    "            early_stopping=True,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "        )\n",
    "\n",
    "    target_sequences = teacher_generation.sequences\n",
    "    teacher_logits = torch.stack(teacher_generation.scores, dim=1)\n",
    "\n",
    "    decoder_input_ids = target_sequences[:, :-1]\n",
    "    student_outputs = student(\n",
    "        input_ids=batch[\"input_ids\"],\n",
    "        attention_mask=batch[\"attention_mask\"],\n",
    "        decoder_input_ids=decoder_input_ids,\n",
    "    )\n",
    "    student_logits = student_outputs.logits\n",
    "\n",
    "    labels = target_sequences[:, 1:]\n",
    "\n",
    "    loss = criterion(\n",
    "        student_logits=student_logits,\n",
    "        teacher_logits=teacher_logits,\n",
    "        targets=labels\n",
    "    )\n",
    "    loss.backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(student.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e3d5cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distillation_epoch(\n",
    "    teacher: BartForConditionalGeneration,\n",
    "    student: BartForConditionalGeneration,\n",
    "    iterator: DataLoader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    device=DEVICE,\n",
    "):\n",
    "\n",
    "    teacher.eval()\n",
    "    student.train()\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    pbar = tqdm(iterator, desc=\"Training\", unit=\"bs\")\n",
    "\n",
    "    for batch_id, batch in enumerate(pbar):\n",
    "        batch = {k: val.to(device) for k, val in batch.items()}\n",
    "        loss = distill_batch(teacher, student, batch, criterion, optimizer, scheduler)\n",
    "        loss_item = loss.item()\n",
    "        losses.append(loss_item)\n",
    "\n",
    "        if batch_id % 100 == 0:\n",
    "            pbar.set_description(f\"Training (loss: {np.mean(losses):.6f})\")\n",
    "\n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e20f68f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate(\n",
    "    model: BartForConditionalGeneration,\n",
    "    tokenizer:AutoTokenizer,\n",
    "    df: pd.DataFrame,\n",
    "    device=DEVICE,\n",
    "):\n",
    "    model.eval()\n",
    "\n",
    "    preds = predict(\n",
    "        df[\"sample\"].tolist(),\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device=device,\n",
    "    )\n",
    "    targets = df[\"lemma\"].tolist()\n",
    "\n",
    "    return lemmatization_accuracy(targets, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "639ab8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.AdamW(student.parameters(), lr=2e-4)\n",
    "# optimizer = torch.optim.SGD(student.parameters(), lr=5e-3)\n",
    "# optimizer = torch.optim.SGD(student.parameters(), lr=5e-4)\n",
    "# optimizer = torch.optim.SGD(student.parameters(), lr=5e-5)\n",
    "optimizer = torch.optim.SGD(student.parameters(), lr=3e-5)\n",
    "# optimizer = torch.optim.AdamW(\n",
    "#     student.parameters(),\n",
    "#     lr=3e-4,\n",
    "#     weight_decay=0.01,\n",
    "#     betas=(0.9, 0.999)\n",
    "# )\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=1)  # ничего не делающий скедьюлер\n",
    "criterion = KLDivAndCELoss(alpha=0.9, temperature=1.3)\n",
    "# criterion = KLDivAndCELoss(alpha=0.9, temperature=1.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7096c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5edfc53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "066df17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1651it [01:33, 17.57it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Acc: 0.9500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/17120 [00:00<?, ?bs/s]You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Training (loss: 0.312932): 100%|██████████| 17120/17120 [1:33:13<00:00,  3.06bs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1651it [01:33, 17.57it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Acc: 0.9500\n"
     ]
    }
   ],
   "source": [
    "highest_val_acc = 0.95\n",
    "patience = 0\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "max_patience = 2\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "\n",
    "    print(f\"Epoch {epoch}/{EPOCHS}\")\n",
    "\n",
    "    val_acc = validate(\n",
    "        student,\n",
    "        tokenizer,\n",
    "        df_dev,\n",
    "        DEVICE,\n",
    "    )\n",
    "\n",
    "    print(f\"Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    train_loss = distillation_epoch(\n",
    "        teacher,\n",
    "        student,\n",
    "        train_dataloader,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        DEVICE,\n",
    "    )\n",
    "\n",
    "    print(f\"Train loss: {train_loss:.4f}\")\n",
    "\n",
    "    val_acc = validate(\n",
    "        student,\n",
    "        tokenizer,\n",
    "        df_dev,\n",
    "        DEVICE,\n",
    "    )\n",
    "\n",
    "    print(f\"Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > highest_val_acc:\n",
    "        torch.save(student.state_dict(), RESULT_MODEL_PATH)\n",
    "        highest_val_acc = val_acc\n",
    "        patience = 0\n",
    "        best_epoch = epoch\n",
    "    else:\n",
    "        patience += 1\n",
    "        print(f\"patience {patience}/{max_patience}\")\n",
    "        if patience == max_patience:\n",
    "            print(\"Early Stopping!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f4ec42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "student.save_pretrained(RESULT_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061ec01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(RESULT_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "87bf1bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1651it [02:57,  9.28it/s]                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Acc: 0.9697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "val_acc = validate(\n",
    "        teacher,\n",
    "        tokenizer,\n",
    "        df_dev,\n",
    "        DEVICE,\n",
    "    )\n",
    "\n",
    "print(f\"Val Acc: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0996bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
